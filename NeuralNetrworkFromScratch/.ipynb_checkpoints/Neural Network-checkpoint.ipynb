{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for FullyConnectedLayer1_W\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer1_B\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for FullyConnectedLayer1_W\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer1_B\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_reg.predict(train_X[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302462, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302991, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302783, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302555, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301829, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302668, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302575, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302867, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302305, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301949, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302622, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301997, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302761, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302407, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301853, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303179, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302647, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300721, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302852, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302514, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f221e432be0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    9000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_with_reg.predict(train_X)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
